# Data Quality Verification Guide

## The Problem

The ML train/val split (train.jsonl vs val.jsonl) is for **preventing overfitting**, NOT for verifying data quality!

```
┌─────────────────────────────────────────────────────────┐
│  BOTH train.jsonl AND val.jsonl need quality checks!    │
│                                                          │
│  Generated by Claude Haiku → Could contain errors:      │
│  ❌ Factual inaccuracies                                │
│  ❌ Hallucinations (made-up facts)                      │
│  ❌ Misinterpretation of legal principles               │
│  ❌ Missing key information                             │
└──────────────────────────────────────────────────────────┘
```

## Verification Strategy

### Level 1: Automated Quality Checks (Fast, catches obvious issues)

```bash
python scripts/automated_quality_checks.py
```

**What it checks:**
- ✓ Output length (too short/long)
- ✓ Refusal patterns ("I cannot", "I don't have")
- ✓ Excessive repetition
- ✓ Missing required fields/format
- ✓ Task-specific requirements (Q&A format, outcome keywords, etc.)

**Limitations:**
- Cannot verify factual accuracy
- May have false positives (like flagging common words)
- Cannot assess legal reasoning quality

**Use case:** Run on ALL 8,000 examples to flag ~10-30% for manual review

---

### Level 2: Manual Spot Checking (Gold standard)

```bash
# Review random sample
python scripts/manual_review_helper.py --sample 50

# Review specific example
python scripts/manual_review_helper.py --cnr DLHC010011762025

# Review validation set only
python scripts/manual_review_helper.py --split val
```

**What to check:**
1. **Factual accuracy:**
   - Dates match source document
   - Parties correctly identified
   - Outcome correctly stated

2. **No hallucination:**
   - All facts present in source
   - No invented citations
   - No made-up legal principles

3. **Legal soundness:**
   - Correct interpretation of law
   - Proper use of legal terminology
   - Logical reasoning

4. **Completeness:**
   - Key information not omitted
   - Appropriate level of detail

**Recommended sampling:**
- **For test (20 examples):** Review ALL
- **For full dataset (8,000):**
  - Review 100% of flagged examples from automated checks
  - Review random 5-10% sample (400-800 examples)
  - Focus on validation set (all 800 val examples if possible)

---

### Level 3: Expert Review (Most thorough)

For production use, have a legal professional review:

1. **Validation set:** ALL 800 val.jsonl examples
2. **Training set sample:** 200-500 random train.jsonl examples
3. **Edge cases:** Complex judgments, constitutional matters, precedent-setting cases

**Why focus on validation set?**
The validation set will be used to evaluate your fine-tuned model. If val.jsonl has errors, you won't know if your model is good or bad!

---

## Practical Verification Workflow

### For Your Current 20 Examples:

```bash
# Step 1: Run automated checks
python scripts/automated_quality_checks.py

# Step 2: Review ALL 20 examples manually (small sample)
python scripts/manual_review_helper.py --sample 20

# Step 3: Specific checks on validation set (only 2 examples)
python scripts/manual_review_helper.py --split val
```

**Time:** ~30-45 minutes for 20 examples

---

### For Full 8,000 Examples:

```bash
# Step 1: Automated checks (5 minutes)
python scripts/automated_quality_checks.py
# Output: Flags ~1,000 examples for review

# Step 2: Review all flagged examples (10-15 hours)
python scripts/manual_review_helper.py --sample 1000

# Step 3: Review random 5% of clean examples (~350 examples, 5-7 hours)
python scripts/manual_review_helper.py --sample 350

# Step 4: Deep review of entire validation set (10-12 hours)
python scripts/manual_review_helper.py --split val
```

**Total time:** ~25-35 hours of manual review

---

## Quality Acceptance Criteria

### Minimum Standards:

| Metric | Threshold | Action if Below |
|--------|-----------|-----------------|
| Factual accuracy | >95% | Regenerate with better prompts |
| No hallucination | >98% | Regenerate with stronger constraints |
| Legal soundness | >90% | Expert review + regeneration |
| Format compliance | 100% | Fix automated validation |

### How to Measure:

From manual review sample of 100 examples:
- 97 factually accurate → 97% ✓
- 2 with minor hallucinations → 98% ✓
- 8 with questionable legal reasoning → 92% ✓
- All properly formatted → 100% ✓

**Result:** PASS - acceptable for training

---

## Common Issues & Solutions

### Issue 1: Hallucinated Citations
**Example:** Output mentions "Section 302 IPC" but source document doesn't reference it

**Detection:** Manual review, keyword search
**Solution:** Improve prompts with "Only use information from the provided judgment"

### Issue 2: Factual Errors
**Example:** Output says "petition allowed" but source says "petition dismissed"

**Detection:** Outcome keyword matching, manual review
**Solution:** Regenerate specific examples, improve prompts

### Issue 3: Missing Context
**Example:** Summary omits key legal principle that was central to case

**Detection:** Manual expert review
**Solution:** Improve prompts to request comprehensive analysis

### Issue 4: Format Issues
**Example:** Research Q&A missing "QUESTION:" or "ANSWER:" markers

**Detection:** Automated checks
**Solution:** Fix formatting with regex or regenerate

---

## Cost-Benefit Analysis

| Verification Level | Time | Cost | Quality Gain |
|-------------------|------|------|--------------|
| Automated only | 5 min | $0 | +20% |
| Automated + 10% manual | 3-4 hours | $0 | +60% |
| Automated + 30% manual | 10-12 hours | $0 | +85% |
| Automated + 50% manual | 18-20 hours | $0 | +95% |
| 100% expert review | 80-100 hours | $5,000+ | +99% |

**Recommendation for Nyay AI:**
- **Minimum:** Automated + 10% manual review
- **Recommended:** Automated + 30% manual review + legal expert spot check
- **Production:** Automated + 50% manual + expert review of val.jsonl

---

## Quick Verification (Current 20 Examples)

Run this now to verify your test data:

```bash
# 1. Check first training example
head -1 data/training/train.jsonl | python3 -m json.tool

# 2. Check first validation example
head -1 data/training/val.jsonl | python3 -m json.tool

# 3. Verify against source
python scripts/manual_review_helper.py --cnr HCBM030212662022
```

Then manually verify:
- ✓ All facts from source document?
- ✓ No invented information?
- ✓ Legally sound interpretation?
- ✓ Clear and complete?

---

## Bottom Line

**You MUST verify both train.jsonl AND val.jsonl!**

The train/val split helps with overfitting, but doesn't guarantee quality. Manual verification is essential for:
1. Ensuring factual accuracy
2. Detecting hallucinations
3. Validating legal reasoning
4. Building trust in your model

For your Nyay AI project, I recommend:
- **Now:** Manually review all 20 test examples (~30 min)
- **Before full run:** Improve prompts if quality issues found
- **After full run:** Automated checks + manual review of 30% (~25 hours)
